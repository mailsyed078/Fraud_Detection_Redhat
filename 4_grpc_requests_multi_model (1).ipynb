{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f73046ff",
   "metadata": {},
   "source": [
    "# GRPC Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443e7e73-24cb-4f03-9491-a6edcc24f0cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup\n",
    "\n",
    "Verify that following variable settings match your deployed model's resource name and grpc URL. The following code assumes that the kube service is in the same namespace, but you could refer to it in full with the namespace, for example: `http://modelmesh-serving.project-name.svc.cluster.local:8008/v2/models/fraud/infer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db9df000-a171-4652-8160-272f81e49612",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: grpcio in /opt/app-root/lib/python3.9/site-packages (1.64.1)\n",
      "Collecting grpcio-tools\n",
      "  Downloading grpcio_tools-1.65.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/app-root/lib/python3.9/site-packages (from grpcio-tools) (69.2.0)\n",
      "Collecting grpcio\n",
      "  Downloading grpcio-1.65.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m143.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting protobuf<6.0dev,>=5.26.1\n",
      "  Downloading protobuf-5.27.3-cp38-abi3-manylinux2014_x86_64.whl (309 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.3/309.3 kB\u001b[0m \u001b[31m298.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf, grpcio, grpcio-tools\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.64.1\n",
      "    Uninstalling grpcio-1.64.1:\n",
      "      Successfully uninstalled grpcio-1.64.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tf2onnx 1.16.1 requires protobuf~=3.20, but you have protobuf 5.27.3 which is incompatible.\n",
      "tensorflow 2.15.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.27.3 which is incompatible.\n",
      "kfp 2.5.0 requires protobuf<4,>=3.13.0, but you have protobuf 5.27.3 which is incompatible.\n",
      "kfp-pipeline-spec 0.2.2 requires protobuf<4,>=3.13.0, but you have protobuf 5.27.3 which is incompatible.\n",
      "kfp-kubernetes 1.0.0 requires protobuf<4,>=3.13.0, but you have protobuf 5.27.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed grpcio-1.65.5 grpcio-tools-1.65.5 protobuf-5.27.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install grpcio grpcio-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d17b252-7827-4cae-adb0-f98c9d80bcd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grpc_host = 'modelmesh-serving'\n",
    "grpc_port = 8033\n",
    "model_name = 'fraud'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4269da9e-5683-4531-9a3f-a1cdad42e3af",
   "metadata": {},
   "source": [
    "## Inspect the gRPC Endpoint\n",
    "\n",
    "Check the gRPC endpoint's model metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "545aa5f4-356f-4e70-b7e6-cd352a68927a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"fraud__isvc-64afb3686a\"\n",
       "versions: \"1\"\n",
       "platform: \"OpenVINO\"\n",
       "inputs {\n",
       "  name: \"dense_input\"\n",
       "  datatype: \"FP32\"\n",
       "  shape: -1\n",
       "  shape: 5\n",
       "}\n",
       "outputs {\n",
       "  name: \"dense_1\"\n",
       "  datatype: \"FP32\"\n",
       "  shape: -1\n",
       "  shape: 1\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./utils')\n",
    "\n",
    "# grpc_predict_v2_pb2 and grpc_predict_v2_pb2_grpc were created from grpc_predict_v2.proto using protoc\n",
    "import grpc\n",
    "import utils.grpc_predict_v2_pb2 as grpc_predict_v2_pb2\n",
    "import utils.grpc_predict_v2_pb2_grpc as grpc_predict_v2_pb2_grpc\n",
    "\n",
    "\n",
    "channel = grpc.insecure_channel(f\"{grpc_host}:{grpc_port}\")\n",
    "stub = grpc_predict_v2_pb2_grpc.GRPCInferenceServiceStub(channel)\n",
    "\n",
    "request = grpc_predict_v2_pb2.ModelMetadataRequest(name=model_name)\n",
    "response = stub.ModelMetadata(request)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5affbf-36c3-4e17-9788-5fc0904de143",
   "metadata": {},
   "source": [
    "### Request Function\n",
    "\n",
    "Build and submit the gRPC request. \n",
    "\n",
    "Note: You submit the data in the same format that you used for an ONNX inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67c1d001-ff99-414a-95d4-5729d5849298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def grpc_request(data):\n",
    "    # request content building\n",
    "    inputs = []\n",
    "    inputs.append(grpc_predict_v2_pb2.ModelInferRequest().InferInputTensor())\n",
    "    inputs[0].name = \"dense_input\"\n",
    "    inputs[0].datatype = \"FP32\"\n",
    "    inputs[0].shape.extend([1, 5])\n",
    "    inputs[0].contents.fp32_contents.extend(data)\n",
    "\n",
    "    # request building\n",
    "    request = grpc_predict_v2_pb2.ModelInferRequest()\n",
    "    request.model_name = model_name\n",
    "    request.inputs.extend(inputs)\n",
    "\n",
    "    response = stub.ModelInfer(request)\n",
    "    result_arr = np.frombuffer(response.raw_output_contents[0], dtype=np.float32)\n",
    "    return result_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911b1015-28b0-4d60-bc17-7b30326b97bc",
   "metadata": {},
   "source": [
    "### Run the Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fc549f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load the scaler\n",
    "import pickle\n",
    "with open('artifact/scaler.pkl', 'rb') as handle:\n",
    "    scaler = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12947866-e0f5-4c72-ba9a-04229b1af990",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.1740077e-05], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [0.3111400080477545, 1.9459399775518593, 1.0, 0.0, 0.0]\n",
    "prediction = grpc_request(scaler.transform([data]).tolist()[0])\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "946f9f1d-b24a-4aa6-b839-f0e8013ef84d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not fraud\n"
     ]
    }
   ],
   "source": [
    "threshhold = 0.95\n",
    "\n",
    "if (prediction[0] > threshhold):\n",
    "    print('fraud')\n",
    "else:\n",
    "    print('not fraud')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7f6b51",
   "metadata": {},
   "source": [
    "## Example 1: user buys a coffee\n",
    "\n",
    "In this example, the user is buying a coffee. The parameters given to the model are:\n",
    "* same location as the last transaction (distance=0)\n",
    "* same median price as the last transaction (ratio_to_median=1)\n",
    "* using a pin number (pin=1)\n",
    "* using the credit card chip (chip=1)\n",
    "* not an online transaction (online=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0a68b67-b109-4a2f-b097-092f4a4d25ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model predicts that this is not fraud\n"
     ]
    }
   ],
   "source": [
    "data = [0.0, 1.0, 1.0, 1.0, 0.0]\n",
    "prediction = grpc_request(scaler.transform([data]).tolist()[0])\n",
    "threshhold = 0.95\n",
    "\n",
    "if (prediction[0] > threshhold):\n",
    "    print('The model predicts that this is fraud')\n",
    "else:\n",
    "    print('The model predicts that this is not fraud')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd27d88",
   "metadata": {},
   "source": [
    "## Example 2: fraudulent transaction\n",
    "\n",
    "In this example, someone stole the user's credit card and is buying something online. The parameters given to the model are:\n",
    "* very far away from the last transaction (distance=100)\n",
    "* median price similar to the last transaction (ratio_to_median=1.2)\n",
    "* not using a pin number (pin=0)\n",
    "* not using the credit card chip (chip=0)\n",
    "* is an online transaction (online=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a736a21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model predicts that this is fraud\n"
     ]
    }
   ],
   "source": [
    "data = [100, 1.2, 0.0, 0.0, 1.0]\n",
    "prediction = grpc_request(scaler.transform([data]).tolist()[0])\n",
    "threshhold = 0.95\n",
    "\n",
    "if (prediction[0] > threshhold):\n",
    "    print('The model predicts that this is fraud')\n",
    "else:\n",
    "    print('The model predicts that this is not fraud')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
